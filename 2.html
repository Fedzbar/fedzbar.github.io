<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>Confusion Matrices and Their Use</title>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
 <!-- Compiled and minified CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">

    <!-- Compiled and minified JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>
</head>
<body>
<div class="container">
<h1>Confusion Matrices and Their Use</h1>
<h3>Introduction</h3>
<p>We need a way to evaluate the performance of our algorithms. To evauate them we can utilize a confusion matrix! For simplicity, I will be discussing a binary 
classification problem. Let's use the typical example of cancer classification: we want to analyze an image and deduce if the tumour is benign or malignant. 

<p>
We will have 4 cases:

<ol>
	<li>Detected malignant, Effectively malignant</li>
	<li>Detected benign, Effectively malignant</li>
	<li>Detected malignant, Effectively benign</li>
	<li>Detected benign, Effectively benign</li>
</ol>

<p>If our goal is to detect malignant tumours, we can name this the "positive" case, while benign will be the "negative" case. Case 1 will be a true positive, while
case 2 will be a false negative. Case 3 will be a false positive, while case 4 will be a true negative. Let's build a matrix with this data:

$$
\begin{bmatrix}
TP & FP \\
FN & TN
\end{bmatrix}
$$

<p>This matrix results very useful when it comes to measuring specific data.

<h3>Accuracy, Recall, Precision and $F_1$ score</h3> 
<p>From the matrix, we can easily calculate different metrics to evaluate the performance of our algorithm.  Accuracy is defined to be:

$$Accuracy = \frac{TP + TN}{\text{Total Cases}}$$

So it simply tells us how many correct classifications we had.  

<p>Let's proceed with recall:

$$Recall = \frac{TP}{TP+FN} = \frac{TP}{\text{Total Actual Positive}}$$

From the formula, we can see that recall effectively tells us the percentage of positive cases which we got correctly. This is very useful in tumour classification
as we want to make sure that malignant tumours are detected. 

<p>Let's proceed with precision: 

$$Precision = \frac{TP}{TP+FP} = \frac{TP}{\text{Total Predicted Positive}}$$

We can see that precision is the ratio between our correct positive predictions and how many we predicted positive. In tumour classification, having high precision
is not as important as having high recall because we care more about detecting as many malignant tumours as possible, even if that means miss-classifying some benign
tumours as malignant.

<p>We can now move on to the $F_1$ score, but first let's talk about the harmonic mean. The harmonic mean $H(x_1,...,x_n)$ is defined to be:

$$\frac{1}{H(x_1,...,x_n)} = \frac{1}{n} \sum \limits_{i=1}^n \frac{1}{x_i}$$

It becomes very useful when wanting the average of rates. The $F_1$ score is simply the harmonic mean between Recall and Precision. Namely: 

$$F_1 = \frac{2 \times precision \times recall}{precision + recall}$$

The $F_1$ score therefore is a compromise between recall and precision.

<h3>Conclusion</h3>
<p>
We have quickly seen how we can utilize the confusion matrix to calculate certain metrics. Knowing this information, we can ultimately decide which metric to give
importance too depending on our case. Accuracy can be useful when missclassification does not have much weight. Recall can be used when missclassification of the
positive class has a lot of weight (tumour classification where classifying a malignant tumour as benign is much worse than the opposite). Precision can be used
when the missclassification of a positive case is bad for example in spam detection where classifying an email as spam incorrectly might lose important information.
$F_1$ is better than accuracy because it gives a better idea of how the classifier is doing as it balances the missclassification cases harmonically.

Next we will be discussing receiver operating characteristic curve and the area under it. 
</div>
</body>
</html>