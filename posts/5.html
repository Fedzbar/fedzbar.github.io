<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">

  <meta name="author" content="Federico Barbero">

  <title>Manifolds and Adversarial Attacks</title>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async>
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
 <!-- Compiled and minified CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">

    <!-- Compiled and minified JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>
</head>
<body>
<div class="container">
    <h3>Manifolds and Adversarial Attacks</h3>
  <h5>Background</h5>
  <p>For a quick introduction to manifolds have a look at some of my previous posts on topology and manifolds. In this article things will start to become very exciting! :D
    All credit goes to the paper <a href="https://arxiv.org/pdf/1811.00525.pdf">Khoury, Hadfield-Menell: On the Geometry of Adversarial Examples</a> <br></p>

<h5>Geometry of Data</h5>
<p>From now on, $k$ will be used to denote the dimension of the manifold $\mathcal{M} \subset \mathbb{R}^n$. The codimension of $\mathcal{M}$ is defined to be: 
$$ \text{codim}(\mathcal{M}) = \dim( \mathbb{R}^n) - \dim(\mathcal{M}) = n - k$$

The "Manifold hypothesis" stems from the observation that data, in practice, is often sampled from manifolds (usually of high codimension).
The data is sampled from different manifolds: $\mathcal{M}_1, ... , \mathcal{M}_C$ and the entire space from which data is sampled (called the data manifold) can be 
thought of as the union of all the manifolds from the different classes: 
$$ \mathcal{M} = \cup_{1 \leq j \leq C} \mathcal{M}_j$$ 

Now define the decision axis $\Lambda_p$ to be the "optimal" decision boundary. That is: the decision axis separates the class manifolds when there is no interesection between them 
and this separation is a large as possible under the metric induced by the canonical p-norm. Now define the reach $\text{rch}_p (T; \mathcal{M})$ for $T \in \mathbb{R}^n$ to be defined as
$\inf_{x \in \mathcal{M}, y \in T} ||x-y||_p$. This is a useful quantity to keep in mind when considering it as: $\text{rch}_p (\Lambda_p; \mathcal{M})$ as in practice it is the smallest distance
between the decision axis and the the data manifold. Finally, let's introduce an $\epsilon$-tubular neighborhoods of $\mathcal{M}$ as: $\mathcal{M}^{\epsilon, p} := \{x \in \mathbb{R}^n | \inf_{y \in \mathcal{M}} ||x-y||_p \leq \epsilon\}$, the
set of points whose distance to the data manifold under the canonical metric induced by the p-norm is less than $\epsilon$. Now we can formally introduce an adversarial example for class $i \in C$ and a classifier 
$f : \mathbb{R}^n \to [C]$ as a point $x \in \mathcal{M}^{\epsilon, p}_i$ such that $f(x) \neq i$. 
<br>
Notice that the decision axis $\Lambda_2$ for example will be different from $\Lambda_{\infty}$ and in general from any other non-equivalent norm. This means that the decision boundary which can be thought of as a non-optimal 
decision axis will have different values of robustness when looking at it with a different norm and norm-induced metric. 
<br>

<h5>"No Free Lunch Theorem" for adversarial defenses under different norms?!?!</h5> 
The paper shows with Theorem 1 that <b>in general no single decison boundary can be robust in all norms</b>, here is a quick sketch of the proof, which is a simple contradiction from a basic example.
<br><br>
<b>Theorem 1.</b> Let $S_1, S_2 \subset \mathcal{R}^{d+1}$ be two concentric d-spheres with radii $r_1 < r_2$ and $S = S_1 \cup S_2$. Then $\Lambda_2 \neq \Lambda_{\infty}$.
<br> 
Proof. The decision axis $\Lambda_2$ is simply a d-sphere which is concentric to $S_1$ and $S_2$ with radius $\frac{r_1 + r_2}{2}$. However, this is clearly not true for $\Lambda_{\infty}$, the geometry of it is infact that of a hyper-cube
which approaches $S_1$ as d increases. 
<br>
These results can be extended to other topological structures such as concentric cylinders and intertwined tori. This is a very important point, Schott et al, attributed the shortcomings of the Madry et al's defense on adversarial attacks to $L_2$ and
$L_0$ perturbations to overfitting on $L_{\infty}$ norm, while this shows that this might not be the case. In fact, what actually might be happening is that learning $\Lambda_{\infty}$ does not necessarily generalize to other decision axes/boundaries. 

<br>
<h5>My Point of View</h5>
This is enough awesomeness for now. Let's take a few steps back and reason on stuff brought up from the paper. First of all, there are two kinds of adversarial examples: low-confidence and high-confidence examples. The former 
are samples which are easier to detect as they lie close to the decision boundary. They are effectively, from a geometric perspective, points for which the classification for the most part is very uncertain. These samples are highly 
decision boundary-dependent. For example we can informally consider the quantity (for a decision boundary $\mathcal{D}$)

$$
\frac{\sum \delta_p}{|\delta_p|}, \text{where } \delta_p := \{\inf_{y \in \Lambda_p} ||x-y||_p \} \text{ } \forall x \in \mathcal{D}
$$

which is intuitively "how far off" on average the decision boundary is from the decision axis. Now changes in this quantity will most certainly affect the set of low confidence adversarial samples. Also notice that the value of p will also 
change the value of this quantity. More importantly if we consider a $\epsilon$-neighborhood of $x \in \mathcal{D}$ embedded in $\mathcal{D}$, this local quantity is even more important, namely: 

$$
\frac{\sum \delta_p^\epsilon(z)}{|\delta_p^\epsilon(z)|}, \text{where } \delta_p^\epsilon(z) := \{\inf_{y \in \Lambda_p} ||x-y||_p \} \text{ } \forall x \in B(z, \epsilon) \cap \mathcal{D}, z \in \mathcal{D} 
$$

As for small $\epsilon$, the volatility of this quantity with respect to different norms is what actually shows the generalized robustness to different adversarial perturbations. Perhaps new work could be aimed in this direction? Maybe.
<br> 
It is of crucial importance to always keep in mind that $\mathcal{M} \subset \mathbb{R}^n$ and for this reason alone, adversarial samples are inevitable as the space $\mathbb{R}^n \setminus \mathcal{M}$ is a total blind spot as there is no information which the
classifier can use. Perhaps trying to make classifiers robust in this entire subspace is impossible and futile, as these are points which do not have any domain-specific meaning. The data manifold is, after all, what we are interested in and notions of distance and semantics
do not go hand in hand in high dimensional space. Overall, this paper introduces a very systematic and elegant formalization of adversarial points which I will keep in mind for my future endeavours in the bizzare realm of high codimensional space.

<br>
Fin :-)

</div>
</body>
</html>
